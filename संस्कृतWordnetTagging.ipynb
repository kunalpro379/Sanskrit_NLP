{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37eEOBg4kqr5",
        "outputId": "1977f0bb-8439-4eeb-e76e-458bfc22b698"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyiwn\n",
            "  Downloading pyiwn-0.0.5-py3-none-any.whl.metadata (778 bytes)\n",
            "Collecting heritage\n",
            "  Downloading heritage-0.1.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from pyiwn) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pyiwn) (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from heritage) (4.13.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.10.0->heritage) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.10.0->heritage) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pyiwn) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pyiwn) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pyiwn) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pyiwn) (2025.10.5)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->pyiwn) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->pyiwn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->pyiwn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->pyiwn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->pyiwn) (1.17.0)\n",
            "Downloading pyiwn-0.0.5-py3-none-any.whl (12 kB)\n",
            "Downloading heritage-0.1.1-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: heritage, pyiwn\n",
            "Successfully installed heritage-0.1.1 pyiwn-0.0.5\n"
          ]
        }
      ],
      "source": [
        "# !pip install pyiwn heritage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyiwn\n",
        "from heritage import HeritagePlatform\n",
        "import re\n",
        "\n",
        "iwn = pyiwn.IndoWordNet(lang=pyiwn.Language.SANSKRIT)\n",
        "platform = HeritagePlatform()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qU_QqfwZomuh",
        "outputId": "2688f5a2-a7a2-4fc3-f4d7-9958524870dc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[██████████████████████████████████████████████████]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:heritage.heritage:Heritage Platform installation not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def analyze_sanskrit_word(word):\n",
        "    \"\"\"Analyze a Sanskrit word and return its root and category\"\"\"\n",
        "    try:\n",
        "        clean_word = re.sub(r'[।॥.,!?;:]', '', word.strip())\n",
        "        if not clean_word:\n",
        "            return None\n",
        "\n",
        "        analysis = platform.get_analysis(clean_word, sentence=False, meta=True)\n",
        "\n",
        "        readable_info = [\n",
        "            {\n",
        "                'Text': word_data['text'],\n",
        "                'Root': word_data.get('root'),\n",
        "                'Category': word_data.get('category')\n",
        "            }\n",
        "            for entry in analysis.values()\n",
        "            for word_list in entry['words']\n",
        "            for word_data in word_list\n",
        "        ]\n",
        "\n",
        "        filtered_roots = [\n",
        "            info['Root'] for info in readable_info\n",
        "            if info['Root'] and not any(char.isdigit() for char in info['Root'])\n",
        "        ]\n",
        "\n",
        "        categories = [\n",
        "            info['Category'] for info in readable_info\n",
        "            if info['Category']\n",
        "        ]\n",
        "\n",
        "        return {\n",
        "            'word': clean_word,\n",
        "            'roots': filtered_roots,\n",
        "            'categories': categories\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing word '{word}': {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "NifEQJbpoqIv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tag_sanskrit_text(text):\n",
        "    words = re.findall(r'[\\u0900-\\u097F]+[।॥]?|[।॥]', text)\n",
        "\n",
        "    tagged_words = []\n",
        "\n",
        "    for word in words:\n",
        "        analysis = analyze_sanskrit_word(word)\n",
        "\n",
        "        if analysis and analysis['roots']:\n",
        "            root = analysis['roots'][0] if analysis['roots'] else 'unknown'\n",
        "            category = analysis['categories'][0] if analysis['categories'] else 'unknown'\n",
        "\n",
        "            tag = f\"<root={root}|category={category}>\"\n",
        "            tagged_word = f\"{word}{tag}\"\n",
        "        else:\n",
        "            tagged_word = f\"{word}<root=unknown|category=unknown>\"\n",
        "\n",
        "        tagged_words.append(tagged_word)\n",
        "\n",
        "    return ' '.join(tagged_words)\n"
      ],
      "metadata": {
        "id": "Udz-5xvFosgV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_sanskrit_file(input_file, output_file):\n",
        "    \"\"\"Process a Sanskrit text file and create tagged output\"\"\"\n",
        "    try:\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            sanskrit_text = f.read()\n",
        "\n",
        "        lines = sanskrit_text.split('\\n')\n",
        "        tagged_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            if line.strip():\n",
        "                tagged_line = tag_sanskrit_text(line)\n",
        "                tagged_lines.append(tagged_line)\n",
        "            else:\n",
        "                tagged_lines.append('')\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(tagged_lines))\n",
        "\n",
        "        print(f\"Successfully processed {input_file}\")\n",
        "        print(f\"Tagged output saved to {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {e}\")\n"
      ],
      "metadata": {
        "id": "QudI7lL2oug9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quick_tag_word(word):\n",
        "    \"\"\"Quick tag a single word\"\"\"\n",
        "    analysis = analyze_sanskrit_word(word)\n",
        "    if analysis and analysis['roots']:\n",
        "        root = analysis['roots'][0]\n",
        "        category = analysis['categories'][0] if analysis['categories'] else 'unknown'\n",
        "        return f\"{word}<root={root}|category={category}>\"\n",
        "    return f\"{word}<root=unknown|category=unknown>\"\n"
      ],
      "metadata": {
        "id": "QGJPOAcWowRt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     process_sanskrit_file('cleaned_output.txt', 'sanskrit_tagged.txt')"
      ],
      "metadata": {
        "id": "F6bFRLJtoyeE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://www.sanskritlibrary.org/downloads/tagged_corpus.zip\n",
        "\n",
        "# !git clone https://github.com/UniversalDependencies/UD_Sanskrit-UFAL\n",
        "\n",
        "# !pip install sanskrit-data\n",
        "# !pip install indic_transliteration"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVQ1JfJBo7-l",
        "outputId": "6ca9202c-cfa7-4941-be20-66597a357ae0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting indic_transliteration\n",
            "  Downloading indic_transliteration-2.3.75-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting backports.functools-lru-cache (from indic_transliteration)\n",
            "  Downloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from indic_transliteration) (2024.11.6)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.12/dist-packages (from indic_transliteration) (0.20.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.12/dist-packages (from indic_transliteration) (0.10.2)\n",
            "Collecting roman (from indic_transliteration)\n",
            "  Downloading roman-5.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting click>=8.0.0 (from typer->indic_transliteration)\n",
            "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from typer->indic_transliteration) (4.15.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer->indic_transliteration) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer->indic_transliteration) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer->indic_transliteration) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer->indic_transliteration) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer->indic_transliteration) (0.1.2)\n",
            "Downloading indic_transliteration-2.3.75-py3-none-any.whl (159 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backports.functools_lru_cache-2.0.0-py2.py3-none-any.whl (6.7 kB)\n",
            "Downloading roman-5.1-py3-none-any.whl (5.8 kB)\n",
            "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: roman, click, backports.functools-lru-cache, indic_transliteration\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 7.1.2\n",
            "    Uninstalling click-7.1.2:\n",
            "      Successfully uninstalled click-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "qtoml 0.2.4 requires click<8.0,>=7.0, but you have click 8.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backports.functools-lru-cache-2.0.0 click-8.3.0 indic_transliteration-2.3.75 roman-5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from indic_transliteration import sanscript\n",
        "from indic_transliteration.sanscript import transliterate\n",
        "\n",
        "def download_vedic_corpus():\n",
        "    \"\"\"Download complete Vedic Sanskrit corpus\"\"\"\n",
        "    base_url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_Sanskrit-Vedic/master/\"\n",
        "\n",
        "    files = [\n",
        "        \"sa_vedic-ud-train.conllu\",\n",
        "        \"sa_vedic-ud-test.conllu\",\n",
        "        \"sa_vedic-ud-dev.conllu\"\n",
        "    ]\n",
        "\n",
        "    all_content = \"\"\n",
        "\n",
        "    for file in files:\n",
        "        try:\n",
        "            url = base_url + file\n",
        "            print(f\"Downloading: {file}\")\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                all_content += response.text + \"\\n\"\n",
        "                print(f\"Downloaded: {len(response.text):,} characters\")\n",
        "            else:\n",
        "                print(f\"Failed: {file} - Status {response.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "    return all_content\n"
      ],
      "metadata": {
        "id": "AFSsEs7JpPlQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def transliterate_to_devanagari(text):\n",
        "    try:\n",
        "        return transliterate(text, sanscript.IAST, sanscript.DEVANAGARI)\n",
        "    except Exception as e:\n",
        "        print(f\" Transliteration warning: {e}\")\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "SOKEjREppPnK"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_conllu_to_tagged_sentences(conllu_text):\n",
        "    sentences = []\n",
        "    current_sentence = []\n",
        "\n",
        "    lines = conllu_text.split('\\n')\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        if not line or line.startswith('#'):\n",
        "            continue\n",
        "\n",
        "        if line == '':\n",
        "            if current_sentence:\n",
        "                tagged_sentence = ' '.join(current_sentence)\n",
        "                sentences.append(tagged_sentence)\n",
        "                current_sentence = []\n",
        "            continue\n",
        "\n",
        "        parts = line.split('\\t')\n",
        "        if len(parts) >= 10:\n",
        "            word_id = parts[0]\n",
        "            form = parts[1]\n",
        "            lemma = parts[2]\n",
        "            upos = parts[3]\n",
        "            xpos = parts[4]\n",
        "            feats = parts[5]\n",
        "            head = parts[6]\n",
        "            deprel = parts[7]\n",
        "            deps = parts[8]\n",
        "            misc = parts[9]\n",
        "\n",
        "            devanagari_form = transliterate_to_devanagari(form)\n",
        "\n",
        "            tagged_word = (\n",
        "                f\"{devanagari_form}\"\n",
        "                f\"<id={word_id}\"\n",
        "                f\"|form={form}\"\n",
        "                f\"|lemma={lemma}\"\n",
        "                f\"|upos={upos}\"\n",
        "                f\"|xpos={xpos}\"\n",
        "                f\"|feats={feats}\"\n",
        "                f\"|head={head}\"\n",
        "                f\"|deprel={deprel}\"\n",
        "                f\"|deps={deps}\"\n",
        "                f\"|misc={misc}>\"\n",
        "            )\n",
        "            current_sentence.append(tagged_word)\n",
        "\n",
        "    if current_sentence:\n",
        "        tagged_sentence = ' '.join(current_sentence)\n",
        "        sentences.append(tagged_sentence)\n",
        "\n",
        "    return sentences\n"
      ],
      "metadata": {
        "id": "o-JMQzvmpPsE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def save_complete_corpus():\n",
        "    \"\"\"Download and save complete Vedic Sanskrit corpus with all linguistic annotations\"\"\"\n",
        "    print(\"Downloading complete Vedic Sanskrit corpus...\")\n",
        "\n",
        "    conllu_text = download_vedic_corpus()\n",
        "\n",
        "    if not conllu_text:\n",
        "        print(\"No data downloaded\")\n",
        "        return\n",
        "\n",
        "    print(\"Parsing and converting Sanskrit words to Devanagari...\")\n",
        "    sentences = parse_conllu_to_tagged_sentences(conllu_text)\n",
        "\n",
        "    output_file = 'vedic_sanskrit_complete_annotated.txt'\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for sent in sentences:\n",
        "            f.write(sent + '\\n')\n",
        "\n",
        "    print(f\"\\nCOMPLETE ANNOTATED CORPUS SAVED!\")\n",
        "    print(f\"Total sentences: {len(sentences):,}\")\n",
        "    print(f\"File: {output_file}\")\n",
        "\n",
        "    show_corpus_statistics(sentences)\n"
      ],
      "metadata": {
        "id": "6Wz_J19BpThT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def show_corpus_statistics(sentences):\n",
        "    \"\"\"Show detailed corpus statistics\"\"\"\n",
        "    total_words = 0\n",
        "    pos_counts = {}\n",
        "\n",
        "    for sent in sentences:\n",
        "        words = sent.split()\n",
        "        total_words += len(words)\n",
        "\n",
        "        for word in words:\n",
        "            if '|upos=' in word:\n",
        "                pos_start = word.find('|upos=') + 6\n",
        "                pos_end = word.find('|', pos_start)\n",
        "                if pos_end == -1:\n",
        "                    pos_end = word.find('>', pos_start)\n",
        "                pos = word[pos_start:pos_end]\n",
        "                pos_counts[pos] = pos_counts.get(pos, 0) + 1\n",
        "\n",
        "    print(f\"\\nCORPUS STATISTICS:\")\n",
        "    print(f\"Total sentences: {len(sentences):,}\")\n",
        "    print(f\"Total words: {total_words:,}\")\n",
        "    print(f\"Unique POS tags: {len(pos_counts)}\")\n",
        "\n",
        "    print(f\"\\nUniversal POS Tag Distribution:\")\n",
        "    for pos, count in sorted(pos_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"  {pos}: {count:,}\")\n",
        "\n",
        "    print(f\"\\nSample sentences with complete annotations:\")\n",
        "    print(\"=\" * 100)\n",
        "    for i, sent in enumerate(sentences[:3], 1):\n",
        "        print(f\"{i}. {sent}\")\n"
      ],
      "metadata": {
        "id": "QkYbA1dIKjw9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def check_dependencies():\n",
        "    \"\"\"Check if required packages are installed\"\"\"\n",
        "    try:\n",
        "        import indic_transliteration\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(\"Required package 'indic-transliteration' not found.\")\n",
        "        print(\"Please install it using: pip install indic-transliteration\")\n",
        "        return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if check_dependencies():\n",
        "        save_complete_corpus()\n",
        "    else:\n",
        "        print(\"Please install the required dependencies and run again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts1C0tuDxywQ",
        "outputId": "6fd24849-60d5-44f0-a075-2a26b74a0d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading complete Vedic Sanskrit corpus...\n",
            "Downloading: sa_vedic-ud-train.conllu\n",
            "Downloaded: 23,071,401 characters\n",
            "Downloading: sa_vedic-ud-test.conllu\n",
            "Downloaded: 2,932,660 characters\n",
            "Downloading: sa_vedic-ud-dev.conllu\n",
            "Downloaded: 3,368,814 characters\n",
            "Parsing and converting Sanskrit words to Devanagari...\n"
          ]
        }
      ]
    }
  ]
}