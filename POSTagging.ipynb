{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "XGijE9IqKvTT"
      },
      "outputs": [],
      "source": [
        "# !pip install requests pyiwn\n",
        "# !pip install sanskritmorph\n",
        "# !pip install sanskrit-lexicon\n",
        "import requests\n",
        "import pyiwn\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "XJwIePP9Lfou"
      },
      "outputs": [],
      "source": [
        "import pyiwn\n",
        "import re\n",
        "# from sanskritmorph import Analyzer\n",
        "\n",
        "iwn = pyiwn.IndoWordNet(lang=pyiwn.Language.SANSKRIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "g27IpWVXLjfw"
      },
      "outputs": [],
      "source": [
        "def get_root_and_pos(word):\n",
        "    try:\n",
        "        synsets = iwn.synsets(word)\n",
        "        for synset in synsets:\n",
        "            lemmas = synset.lemmas()\n",
        "            if lemmas:\n",
        "                root = lemmas[0].name()\n",
        "                pos = str(synset.pos())\n",
        "                if pos != 'None':\n",
        "                    return root, pos\n",
        "        return word, 'UNKNOWN'\n",
        "    except:\n",
        "        return word, 'UNKNOWN'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "T1P2OKOlOpxl"
      },
      "outputs": [],
      "source": [
        "def get_root_from_iwn(word):\n",
        "    try:\n",
        "        synsets = iwn.synsets(word)\n",
        "        for synset in synsets:\n",
        "            lemmas = synset.lemmas()\n",
        "            if lemmas:\n",
        "                return lemmas[0].name()\n",
        "        return 'unknown'\n",
        "    except:\n",
        "        return 'unknown'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "uL4NNUZRLk0N"
      },
      "outputs": [],
      "source": [
        "def process_word(word):\n",
        "    if word in ['।', '॥']:\n",
        "        return {'text': word, 'root': 'punctuation', 'category': 'PUNC'}\n",
        "\n",
        "    clean_word = re.sub(r'[।॥.,!?;:]', '', word.strip())\n",
        "    if not clean_word:\n",
        "        return {'text': word, 'root': 'punctuation', 'category': 'PUNC'}\n",
        "\n",
        "    root, pos = get_root_and_pos(clean_word)\n",
        "\n",
        "    return {\n",
        "        'text': word,\n",
        "        'root': root,\n",
        "        'category': pos\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "yFGjsBDTLsuI"
      },
      "outputs": [],
      "source": [
        "def tag_text(text):\n",
        "    words = re.findall(r'[\\u0900-\\u097F]+[।॥]?|[।॥]', text)\n",
        "    tagged_words = []\n",
        "\n",
        "    for word in words:\n",
        "        analysis = process_word(word)\n",
        "        tagged_words.append(\n",
        "            f\"{analysis['text']}<root={analysis['root']}|pos={analysis['category']}>\"\n",
        "        )\n",
        "\n",
        "    return ' '.join(tagged_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "lz-k5ZvQOu8T"
      },
      "outputs": [],
      "source": [
        "def process_file(input_file, output_file):\n",
        "    try:\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            for line in lines:\n",
        "                if line.strip():\n",
        "                    tagged_line = tag_text(line.strip())\n",
        "                    f.write(f\"{tagged_line}\\n\")\n",
        "                else:\n",
        "                    f.write('\\n')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9U4pc3iP5KS",
        "outputId": "e86a5977-5adf-4127-a015-4ccaedec5870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "आत्मा: root=आत्मा-उपनिषद्, pos=noun\n",
            "सर्वत्र: root=प्रतिस्थानम्, pos=adverb\n",
            "व्याप्यते: root=व्याप्यते, pos=UNKNOWN\n",
            "मनः: root=मतम्, pos=noun\n"
          ]
        }
      ],
      "source": [
        "test_words = [\"आत्मा\", \"सर्वत्र\", \"व्याप्यते\", \"मनः\"]\n",
        "for word in test_words:\n",
        "    result = process_word(word)\n",
        "    print(f\"{word}: root={result['root']}, pos={result['category']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "8zYK4f7bQv2o"
      },
      "outputs": [],
      "source": [
        "process_file('data.txt', 'sanskrit_pos_output.txt')\n",
        "# !pip install sanskrit_parser\n",
        "# !pip install git+https://github.com/avinashvarna/sanskrit_parser.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "i1rfUFSQRukL"
      },
      "outputs": [],
      "source": [
        "import pyiwn\n",
        "import re\n",
        "from sanskrit_parser import Parser\n",
        "\n",
        "iwn = pyiwn.IndoWordNet(lang=pyiwn.Language.SANSKRIT)\n",
        "parser = Parser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "nQ1_0lh4R2GP"
      },
      "outputs": [],
      "source": [
        "def get_root_and_pos_with_parser(word):\n",
        "    try:\n",
        "        result = parser.parse(word)\n",
        "        if result and len(result) > 0:\n",
        "            analysis = result[0]\n",
        "            root = analysis.get_root()\n",
        "            pos = analysis.get_pos()\n",
        "            if root and pos:\n",
        "                return root, pos\n",
        "    except:\n",
        "        pass\n",
        "    return word, 'UNKNOWN'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "H1jhjoX0SLF6"
      },
      "outputs": [],
      "source": [
        "def get_root_and_pos_with_iwn(word):\n",
        "    try:\n",
        "        synsets = iwn.synsets(word)\n",
        "        for synset in synsets:\n",
        "            lemmas = synset.lemmas()\n",
        "            if lemmas:\n",
        "                root = lemmas[0].name()\n",
        "                pos = str(synset.pos())\n",
        "                if pos != 'None':\n",
        "                    return root, pos\n",
        "        return word, 'UNKNOWN'\n",
        "    except:\n",
        "        return word, 'UNKNOWN'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "5eAS7Y8eSMXJ"
      },
      "outputs": [],
      "source": [
        "def process_word(word):\n",
        "    if word in ['।', '॥']:\n",
        "        return {'text': word, 'root': 'punctuation', 'category': 'PUNC'}\n",
        "\n",
        "    clean_word = re.sub(r'[।॥.,!?;:]', '', word.strip())\n",
        "    if not clean_word:\n",
        "        return {'text': word, 'root': 'punctuation', 'category': 'PUNC'}\n",
        "\n",
        "    root, pos = get_root_and_pos_with_parser(clean_word)\n",
        "\n",
        "    if pos == 'UNKNOWN':\n",
        "        root, pos = get_root_and_pos_with_iwn(clean_word)\n",
        "\n",
        "    return {\n",
        "        'text': word,\n",
        "        'root': root,\n",
        "        'category': pos\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "vgZZfErbSNgK"
      },
      "outputs": [],
      "source": [
        "def tag_text(text):\n",
        "    words = re.findall(r'[\\u0900-\\u097F]+[।॥]?|[।॥]', text)\n",
        "    tagged_words = []\n",
        "\n",
        "    for word in words:\n",
        "        analysis = process_word(word)\n",
        "        tagged_words.append(\n",
        "            f\"{analysis['text']}<root={analysis['root']}|pos={analysis['category']}>\"\n",
        "        )\n",
        "\n",
        "    return ' '.join(tagged_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "TZNLkUh6SO0x"
      },
      "outputs": [],
      "source": [
        "def process_file(input_file, output_file):\n",
        "    try:\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            for line in lines:\n",
        "                if line.strip():\n",
        "                    tagged_line = tag_text(line.strip())\n",
        "                    f.write(f\"{tagged_line}\\n\")\n",
        "                else:\n",
        "                    f.write('\\n')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtvjZQS3SP1e",
        "outputId": "addd7618-8242-4516-f1b1-051b73c12266"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "व्याप्यते: root=व्याप्यते, pos=UNKNOWN\n",
            "करोति: root=करोति, pos=UNKNOWN\n",
            "भवति: root=भवति, pos=UNKNOWN\n",
            "गच्छति: root=गच्छति, pos=UNKNOWN\n"
          ]
        }
      ],
      "source": [
        "test_words = [\"व्याप्यते\", \"करोति\", \"भवति\", \"गच्छति\"]\n",
        "for word in test_words:\n",
        "    result = process_word(word)\n",
        "    print(f\"{word}: root={result['root']}, pos={result['category']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "W0mvU0KmSTrW"
      },
      "outputs": [],
      "source": [
        "# !pip install sanskrit-data\n",
        "# !pip install sanskrit-util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "LkFr9mfESWB-"
      },
      "outputs": [],
      "source": [
        "def get_root_and_pos(word):\n",
        "    try:\n",
        "        synsets = iwn.synsets(word)\n",
        "        for synset in synsets:\n",
        "            lemmas = synset.lemmas()\n",
        "            if lemmas:\n",
        "                root = lemmas[0].name()\n",
        "                pos = str(synset.pos())\n",
        "                if pos != 'None':\n",
        "                    return root, pos\n",
        "        return word, 'UNKNOWN'\n",
        "    except:\n",
        "        return word, 'UNKNOWN'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "4OTDciz7SYIU"
      },
      "outputs": [],
      "source": [
        "def process_word(word):\n",
        "    if word in ['।', '॥']:\n",
        "        return {'text': word, 'root': 'punctuation', 'category': 'PUNC'}\n",
        "\n",
        "    clean_word = re.sub(r'[।॥.,!?;:]', '', word.strip())\n",
        "    if not clean_word:\n",
        "        return {'text': word, 'root': 'punctuation', 'category': 'PUNC'}\n",
        "\n",
        "    root, pos = get_root_and_pos(clean_word)\n",
        "\n",
        "    return {\n",
        "        'text': word,\n",
        "        'root': root,\n",
        "        'category': pos\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "0-LWHN6kTJTz"
      },
      "outputs": [],
      "source": [
        "def tag_text(text):\n",
        "    words = re.findall(r'[\\u0900-\\u097F]+[।॥]?|[।॥]', text)\n",
        "    tagged_words = []\n",
        "\n",
        "    for word in words:\n",
        "        analysis = process_word(word)\n",
        "        tagged_words.append(\n",
        "            f\"{analysis['text']}<root={analysis['root']}|pos={analysis['category']}>\"\n",
        "        )\n",
        "\n",
        "    return ' '.join(tagged_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "3d4rfXE9TKed"
      },
      "outputs": [],
      "source": [
        "def process_file(input_file, output_file):\n",
        "    try:\n",
        "        with open(input_file, 'r', encoding='utf-8') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            for line in lines:\n",
        "                if line.strip():\n",
        "                    tagged_line = tag_text(line.strip())\n",
        "                    f.write(f\"{tagged_line}\\n\")\n",
        "                else:\n",
        "                    f.write('\\n')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "_ELKIrwkTLwf"
      },
      "outputs": [],
      "source": [
        "process_file('data.txt', 'sanskrit_output.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "cO_A2xv8TajT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "LRUW-dAdTc5J"
      },
      "outputs": [],
      "source": [
        "# !wget https://www.sanskritlibrary.org/downloads/tagged_corpus.zip\n",
        "\n",
        "# !git clone https://github.com/UniversalDependencies/UD_Sanskrit-UFAL\n",
        "\n",
        "# !pip install sanskrit-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "9a32uQNXUcOU",
        "outputId": "5941b650-bf44-4531-d75e-ed661cc73705"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1\\tरामः\\tराम\\tNOUN\\t_\\tCase=Nom|Gender=Masc|Number=Sing\\t0\\t_\\t_\\t_\\n2\\tवनम्\\tवन\\tNOUN\\t_\\tCase=Acc|Gender=Neut|Number=Sing\\t3\\tobj\\t_\\t_\\n3\\tगच्छति\\tगम्\\tVERB\\t_\\tMood=Ind|Number=Sing|Person=3|Tense=Pres|Voice=Act\\t0\\t_\\t_\\t_\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 181
        }
      ],
      "source": [
        "# UoH Sanskrit Treebank example\n",
        "def load_ud_sanskrit():\n",
        "    \"\"\"Load Universal Dependencies Sanskrit data\"\"\"\n",
        "    try:\n",
        "        with open('UD_Sanskrit-UFAL/sa_ufal-ud-train.conllu', 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "    except:\n",
        "        return \"Dataset not found\"\n",
        "\n",
        "# Sample of what you'll get:\n",
        "\"\"\"\n",
        "1\tरामः\tराम\tNOUN\t_\tCase=Nom|Gender=Masc|Number=Sing\t0\t_\t_\t_\n",
        "2\tवनम्\tवन\tNOUN\t_\tCase=Acc|Gender=Neut|Number=Sing\t3\tobj\t_\t_\n",
        "3\tगच्छति\tगम्\tVERB\t_\tMood=Ind|Number=Sing|Person=3|Tense=Pres|Voice=Act\t0\t_\t_\t_\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iJSOMnsUf9s",
        "outputId": "b7e3f2b4-161b-4842-cf9b-c5152286c577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: sa_vedic-ud-test.conllu\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def download_sanskrit_pos_data():\n",
        "    \"\"\"Download pre-tagged Sanskrit data\"\"\"\n",
        "    urls = [\n",
        "        \"https://raw.githubusercontent.com/UniversalDependencies/UD_Sanskrit-UFAL/master/sa_ufal-ud-train.conllu\",\n",
        "        \"https://raw.githubusercontent.com/UniversalDependencies/UD_Sanskrit-Vedic/master/sa_vedic-ud-test.conllu\"\n",
        "    ]\n",
        "\n",
        "    for url in urls:\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                filename = url.split('/')[-1]\n",
        "                with open(filename, 'w', encoding='utf-8') as f:\n",
        "                    f.write(response.text)\n",
        "                print(f\"Downloaded: {filename}\")\n",
        "        except:\n",
        "            print(f\"Failed: {url}\")\n",
        "\n",
        "download_sanskrit_pos_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2fbASVAUi1U",
        "outputId": "c00e7024-a92b-42aa-9661-cec3bc03a5fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "पञ्चतन्त्रम्<root=पञ्चतन्त्र|pos=PROPN>\n",
            "कथामुखम्<root=कथामुख|pos=NOUN>\n",
            "ओं<root=ओं|pos=INTJ>\n",
            "नमः<root=नमस्|pos=NOUN>\n",
            "श्रीशारदागणपतिगुरुभ्यः<root=_|pos=_>\n",
            "श्री<root=श्री|pos=ADJ>\n",
            "शारदा<root=शारदा|pos=PROPN>\n",
            "गणपति<root=गणपति|pos=PROPN>\n",
            "गुरुभ्यः<root=गुरु|pos=NOUN>\n",
            "।<root=।|pos=PUNCT>\n",
            "महाकविभ्यो<root=_|pos=_>\n",
            "महा<root=महत्|pos=NOUN>\n",
            "कविभ्यो<root=कवि|pos=NOUN>\n",
            "नमः<root=नमस्|pos=NOUN>\n",
            "।<root=।|pos=PUNCT>\n",
            "ब्रह्मा<root=ब्रह्मन्|pos=PROPN>\n",
            "रुद्रः<root=रुद्र|pos=PROPN>\n",
            "कुमारो<root=कुमार|pos=PROPN>\n",
            "हरिवरुणयमा<root=_|pos=_>\n",
            "हरि<root=हरि|pos=PROPN>\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "def download_and_show_tagged_data():\n",
        "    url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_Sanskrit-UFAL/master/sa_ufal-ud-test.conllu\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            lines = response.text.split('\\n')\n",
        "            tagged_examples = []\n",
        "\n",
        "            for line in lines:\n",
        "                if line.strip() and not line.startswith('#'):\n",
        "                    parts = line.split('\\t')\n",
        "                    if len(parts) >= 4:\n",
        "                        word = parts[1]\n",
        "                        pos = parts[3]\n",
        "                        lemma = parts[2]\n",
        "                        tagged_examples.append(f\"{word}<root={lemma}|pos={pos}>\")\n",
        "\n",
        "                        if len(tagged_examples) >= 20:\n",
        "                            break\n",
        "\n",
        "            return tagged_examples\n",
        "        else:\n",
        "            return [\"Download failed\"]\n",
        "    except Exception as e:\n",
        "        return [f\"Error: {e}\"]\n",
        "\n",
        "tagged_data = download_and_show_tagged_data()\n",
        "for item in tagged_data:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "9BbGW0IgVCuc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "\n",
        "def download_and_extract_ud_dataset(lang_name, zip_url):\n",
        "    print(f\"\\nDownloading {lang_name} dataset...\")\n",
        "    response = requests.get(zip_url, timeout=60)\n",
        "    if response.status_code != 200:\n",
        "        print(f\" Failed to download {lang_name} ({response.status_code})\")\n",
        "        return None\n",
        "\n",
        "    folder_name = lang_name.replace(\"-\", \"_\") + \"_data\"\n",
        "    os.makedirs(folder_name, exist_ok=True)\n",
        "\n",
        "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
        "        z.extractall(folder_name)\n",
        "    print(f\" Extracted {lang_name} to {folder_name}/\")\n",
        "    return folder_name\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "SSkjKaLMvtPT"
      },
      "outputs": [],
      "source": [
        "\n",
        "def parse_conllu_to_tagged(conllu_text):\n",
        "    sentences = []\n",
        "    current_sentence = []\n",
        "    for line in conllu_text.split('\\n'):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            if current_sentence:\n",
        "                sentences.append(' '.join(current_sentence))\n",
        "                current_sentence = []\n",
        "            continue\n",
        "        if line.startswith(\"#\"):\n",
        "            continue\n",
        "        parts = line.split('\\t')\n",
        "        if len(parts) >= 4:\n",
        "            word, lemma, pos = parts[1], parts[2], parts[3]\n",
        "            if re.search(r'[\\u0900-\\u097F]', word):\n",
        "                current_sentence.append(f\"{word}<root={lemma}|pos={pos}>\")\n",
        "    if current_sentence:\n",
        "        sentences.append(' '.join(current_sentence))\n",
        "    return sentences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "MPpHUuimv3DQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def load_conllu_from_folder(folder_path):\n",
        "    sentences = []\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".conllu\"):\n",
        "                path = os.path.join(root, file)\n",
        "                try:\n",
        "                    with open(path, 'r', encoding='utf-8') as f:\n",
        "                        text = f.read()\n",
        "                    sents = parse_conllu_to_tagged(text)\n",
        "                    sentences.extend(sents)\n",
        "                    print(f\"Parsed {len(sents)} from {file}\")\n",
        "                except Exception as e:\n",
        "                    print(f\" Error reading {file}: {e}\")\n",
        "    return sentences\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "I5cdpDaMv5o5"
      },
      "outputs": [],
      "source": [
        "\n",
        "def show_corpus_stats(sentences):\n",
        "    total_words = 0\n",
        "    pos_distribution = {}\n",
        "\n",
        "    for sent in sentences:\n",
        "        words = sent.split()\n",
        "        total_words += len(words)\n",
        "        for word in words:\n",
        "            m = re.search(r'\\|pos=([^>]+)', word)\n",
        "            if m:\n",
        "                pos = m.group(1)\n",
        "                pos_distribution[pos] = pos_distribution.get(pos, 0) + 1\n",
        "\n",
        "    print(f\"\\n CORPUS STATISTICS:\")\n",
        "    print(f\"Total sentences: {len(sentences):,}\")\n",
        "    print(f\"Total words: {total_words:,}\")\n",
        "    print(f\"Unique POS tags: {len(pos_distribution)}\")\n",
        "\n",
        "    print(\"\\n POS Tag Distribution:\")\n",
        "    for pos, count in sorted(pos_distribution.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
        "        print(f\"  {pos}: {count:,}\")\n",
        "\n",
        "\n",
        "    # for i, sent in enumerate(sentences[:3], 1):\n",
        "    #     print(f\"{i}. {sent}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwWIVugPv8CA",
        "outputId": "e506ca99-7808-4315-b323-faf58b38597c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading UD_Sanskrit-UFAL dataset...\n",
            " Extracted UD_Sanskrit-UFAL to UD_Sanskrit_UFAL_data/\n",
            "Parsed 230 from sa_ufal-ud-test.conllu\n",
            "Parsed 1999 from parsed.conllu\n",
            "Parsed 1569 from combined.conllu\n",
            "\n",
            "Downloading UD_Hindi-HDTB dataset...\n",
            " Extracted UD_Hindi-HDTB to UD_Hindi_HDTB_data/\n",
            "Parsed 1659 from hi_hdtb-ud-dev.conllu\n",
            "Parsed 13306 from hi_hdtb-ud-train.conllu\n",
            "Parsed 1684 from hi_hdtb-ud-test.conllu\n",
            "\n",
            "SUCCESS! Combined corpus saved to: /content/indic_large_tagged_corpus.txt\n",
            "Total pre-tagged sentences: 20,447\n",
            "\n",
            "\n",
            " CORPUS STATISTICS:\n",
            "Total sentences: 20,447\n",
            "Total words: 383,765\n",
            "Unique POS tags: 17\n",
            "\n",
            " POS Tag Distribution:\n",
            "  NOUN: 90,479\n",
            "  ADP: 73,221\n",
            "  PROPN: 43,082\n",
            "  VERB: 39,581\n",
            "  ADJ: 25,426\n",
            "  AUX: 23,772\n",
            "  PUNCT: 19,224\n",
            "  PRON: 17,672\n",
            "  X: 9,008\n",
            "  PART: 8,856\n",
            "  DET: 7,995\n",
            "  CCONJ: 7,133\n",
            "  SCONJ: 6,819\n",
            "  NUM: 6,354\n",
            "  ADV: 4,876\n",
            "  _: 259\n",
            "  INTJ: 8\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "def build_large_indic_corpus():\n",
        "    datasets = {\n",
        "        \"UD_Sanskrit-UFAL\": \"https://github.com/UniversalDependencies/UD_Sanskrit-UFAL/archive/refs/tags/r2.12.zip\",\n",
        "        \"UD_Hindi-HDTB\": \"https://github.com/UniversalDependencies/UD_Hindi-HDTB/archive/refs/tags/r2.12.zip\",\n",
        "    }\n",
        "\n",
        "    all_sentences = []\n",
        "    for name, url in datasets.items():\n",
        "        folder = download_and_extract_ud_dataset(name, url)\n",
        "        if folder:\n",
        "            sentences = load_conllu_from_folder(folder)\n",
        "            all_sentences.extend(sentences)\n",
        "\n",
        "    if not all_sentences:\n",
        "        print(\" No sentences found. Exiting.\")\n",
        "        return\n",
        "\n",
        "    out_file = \"indic_large_tagged_corpus.txt\"\n",
        "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for sent in all_sentences:\n",
        "            f.write(sent + \"\\n\")\n",
        "\n",
        "    print(f\"\\nSUCCESS! Combined corpus saved to: {os.path.abspath(out_file)}\")\n",
        "    print(f\"Total pre-tagged sentences: {len(all_sentences):,}\\n\")\n",
        "    show_corpus_stats(all_sentences)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    build_large_indic_corpus()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "p8Rl8DXPgs9a"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from indic_transliteration import sanscript\n",
        "from indic_transliteration.sanscript import transliterate\n",
        "\n",
        "def download_vedic_corpus():\n",
        "    \"\"\"Download complete Vedic Sanskrit corpus\"\"\"\n",
        "    base_url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_Sanskrit-Vedic/master/\"\n",
        "\n",
        "    files = [\n",
        "        \"sa_vedic-ud-train.conllu\",\n",
        "        \"sa_vedic-ud-test.conllu\",\n",
        "        \"sa_vedic-ud-dev.conllu\"\n",
        "    ]\n",
        "\n",
        "    all_content = \"\"\n",
        "\n",
        "    for file in files:\n",
        "        try:\n",
        "            url = base_url + file\n",
        "            print(f\"Downloading: {file}\")\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                all_content += response.text + \"\\n\"\n",
        "                print(f\"Downloaded: {len(response.text):,} characters\")\n",
        "            else:\n",
        "                print(f\"Failed: {file} - Status {response.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "    return all_content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "SZocUW3VwRKF"
      },
      "outputs": [],
      "source": [
        "\n",
        "def transliterate_to_devanagari(text):\n",
        "    try:\n",
        "        return transliterate(text, sanscript.IAST, sanscript.DEVANAGARI)\n",
        "    except Exception as e:\n",
        "        print(f\" Transliteration warning: {e}\")\n",
        "        return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "WYEKeal6wVCo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def parse_conllu_to_tagged_sentences(conllu_text):\n",
        "    sentences = []\n",
        "    current_sentence = []\n",
        "\n",
        "    lines = conllu_text.split('\\n')\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "\n",
        "        if not line or line.startswith('#'):\n",
        "            continue\n",
        "\n",
        "        if line == '':\n",
        "            if current_sentence:\n",
        "                tagged_sentence = ' '.join(current_sentence)\n",
        "                sentences.append(tagged_sentence)\n",
        "                current_sentence = []\n",
        "            continue\n",
        "\n",
        "        parts = line.split('\\t')\n",
        "        if len(parts) >= 10:\n",
        "            word_id = parts[0]\n",
        "            form = parts[1]\n",
        "            lemma = parts[2]\n",
        "            upos = parts[3]\n",
        "            xpos = parts[4]\n",
        "            feats = parts[5]\n",
        "            head = parts[6]\n",
        "            deprel = parts[7]\n",
        "            deps = parts[8]\n",
        "            misc = parts[9]\n",
        "\n",
        "            devanagari_form = transliterate_to_devanagari(form)\n",
        "\n",
        "            tagged_word = (\n",
        "                f\"{devanagari_form}\"\n",
        "                f\"<id={word_id}\"\n",
        "                f\"|form={form}\"\n",
        "                f\"|lemma={lemma}\"\n",
        "                f\"|upos={upos}\"\n",
        "                f\"|xpos={xpos}\"\n",
        "                f\"|feats={feats}\"\n",
        "                f\"|head={head}\"\n",
        "                f\"|deprel={deprel}\"\n",
        "                f\"|deps={deps}\"\n",
        "                f\"|misc={misc}>\"\n",
        "            )\n",
        "            current_sentence.append(tagged_word)\n",
        "\n",
        "    if current_sentence:\n",
        "        tagged_sentence = ' '.join(current_sentence)\n",
        "        sentences.append(tagged_sentence)\n",
        "\n",
        "    return sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "pXDPr4acwX9s"
      },
      "outputs": [],
      "source": [
        "\n",
        "def save_complete_corpus():\n",
        "\n",
        "\n",
        "    conllu_text = download_vedic_corpus()\n",
        "\n",
        "    if not conllu_text:\n",
        "        print(\"No data downloaded\")\n",
        "        return\n",
        "\n",
        "    print(\"Parsing and converting Sanskrit words to Devanagari...\")\n",
        "    sentences = parse_conllu_to_tagged_sentences(conllu_text)\n",
        "\n",
        "    output_file = 'vedic_sanskrit_complete_annotated.txt'\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for sent in sentences:\n",
        "            f.write(sent + '\\n')\n",
        "\n",
        "    print(f\"\\nCOMPLETE ANNOTATED CORPUS SAVED!\")\n",
        "    print(f\"Total sentences: {len(sentences):,}\")\n",
        "    print(f\"File: {output_file}\")\n",
        "\n",
        "    show_corpus_statistics(sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "Lun31jxtwaEm"
      },
      "outputs": [],
      "source": [
        "\n",
        "def show_corpus_statistics(sentences):\n",
        "    \"\"\"Show detailed corpus statistics\"\"\"\n",
        "    total_words = 0\n",
        "    pos_counts = {}\n",
        "\n",
        "    for sent in sentences:\n",
        "        words = sent.split()\n",
        "        total_words += len(words)\n",
        "\n",
        "        for word in words:\n",
        "            if '|upos=' in word:\n",
        "                pos_start = word.find('|upos=') + 6\n",
        "                pos_end = word.find('|', pos_start)\n",
        "                if pos_end == -1:\n",
        "                    pos_end = word.find('>', pos_start)\n",
        "                pos = word[pos_start:pos_end]\n",
        "                pos_counts[pos] = pos_counts.get(pos, 0) + 1\n",
        "\n",
        "    print(f\"\\nCORPUS STATISTICS:\")\n",
        "    print(f\"Total sentences: {len(sentences):,}\")\n",
        "    print(f\"Total words: {total_words:,}\")\n",
        "    print(f\"Unique POS tags: {len(pos_counts)}\")\n",
        "\n",
        "    print(f\"\\nUniversal POS Tag Distribution:\")\n",
        "    for pos, count in sorted(pos_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"  {pos}: {count:,}\")\n",
        "\n",
        "    # print(f\"\\nSample sentences with complete annotations:\")\n",
        "\n",
        "    # for i, sent in enumerate(sentences[:3], 1):\n",
        "    #     print(f\"{i}. {sent}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWHyhJUBwcim",
        "outputId": "d2b070c1-30d4-4322-c8cb-0bb90ec581e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: sa_vedic-ud-train.conllu\n",
            "Downloaded: 23,071,401 characters\n",
            "Downloading: sa_vedic-ud-test.conllu\n",
            "Downloaded: 2,932,660 characters\n",
            "Downloading: sa_vedic-ud-dev.conllu\n",
            "Downloaded: 3,368,814 characters\n",
            "Parsing and converting Sanskrit words to Devanagari...\n",
            "\n",
            "COMPLETE ANNOTATED CORPUS SAVED!\n",
            "Total sentences: 1\n",
            "File: vedic_sanskrit_complete_annotated.txt\n",
            "\n",
            "CORPUS STATISTICS:\n",
            "Total sentences: 1\n",
            "Total words: 206,440\n",
            "Unique POS tags: 13\n",
            "\n",
            "Universal POS Tag Distribution:\n",
            "  NOUN: 72,315\n",
            "  VERB: 39,836\n",
            "  PRON: 27,825\n",
            "  PART: 21,136\n",
            "  ADJ: 18,315\n",
            "  ADV: 13,863\n",
            "  CCONJ: 4,222\n",
            "  NUM: 2,900\n",
            "  AUX: 1,825\n",
            "  SCONJ: 1,730\n",
            "  ADP: 1,383\n",
            "  DET: 565\n",
            "  INTJ: 525\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def check_dependencies():\n",
        "    \"\"\"Check if required packages are installed\"\"\"\n",
        "    try:\n",
        "        import indic_transliteration\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(\"Required package 'indic-transliteration' not found.\")\n",
        "        print(\"Please install it using: pip install indic-transliteration\")\n",
        "        return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if check_dependencies():\n",
        "        save_complete_corpus()\n",
        "    else:\n",
        "        print(\"Please install the required dependencies and run again.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}